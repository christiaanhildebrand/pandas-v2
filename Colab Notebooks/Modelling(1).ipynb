{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOboGgIJZ+PFCF0zmZsKeB4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers, models, callbacks\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.losses import Huber\n","from sklearn.preprocessing import StandardScaler\n","import random\n","import pickle\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load your train_df\n","train_df = pd.read_csv('/content/drive/MyDrive/Modelling/train_df.csv')  # Change to your path\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLICXbgUOyJg","executionInfo":{"status":"ok","timestamp":1723717127364,"user_tz":-120,"elapsed":4937,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}},"outputId":"7c6de8fb-082f-48a1-a498-a80b4f44cfb0"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"t366k6StKZVI","executionInfo":{"status":"ok","timestamp":1723714518451,"user_tz":-120,"elapsed":11729,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}}},"outputs":[],"source":["def load_and_preprocess_image(image_path, target_size=(224, 224)):\n","    image = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n","    image = tf.keras.preprocessing.image.img_to_array(image)\n","    image = image / 255.0  # Normalize to [0, 1]\n","    return image\n","\n","# Prepare image data and scalar features\n","image_dir = '/content/drive/MyDrive/Modelling/valid_images'\n","image_paths = []\n","scalar_features = []\n","labels = []\n","\n","for index, row in train_df.iterrows():\n","    pothole_id = row['pothole_id']\n","    image_path = os.path.join(image_dir, f'{pothole_id}.jpg')\n","\n","    if os.path.exists(image_path):\n","        image_paths.append(image_path)\n","        scalar_features.append([row['pothole_area_mm2']])\n","        labels.append(row['bags_used'])\n","\n","# Convert lists to numpy arrays\n","images = np.array([load_and_preprocess_image(path) for path in image_paths])\n","scalar_features = np.array(scalar_features)\n","labels = np.array(labels)\n","\n","# Scale the scalar features\n","scaler = StandardScaler()\n","scalar_features = scaler.fit_transform(scalar_features)\n","\n","# Save the scaler for later use\n","scaler_path = '/content/drive/MyDrive/Modelling/scaler.pkl'\n","with open(scaler_path, 'wb') as f:\n","    pickle.dump(scaler, f)"]},{"cell_type":"markdown","source":["# Embedding Layer"],"metadata":{"id":"ZQ13RH2-eV4Z"}},{"cell_type":"code","source":["def create_complex_cnn_model(input_shape):\n","    model = models.Sequential([\n","        layers.Input(shape=input_shape),\n","        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.GlobalAveragePooling2D(),\n","        layers.Dense(512, activation='relu'),\n","        layers.Dropout(0.5),  # Add dropout to reduce overfitting\n","        layers.Dense(256, activation='relu'),\n","    ])\n","    return model\n","\n","def create_contrastive_model(input_shape):\n","    base_model = create_complex_cnn_model(input_shape)\n","\n","    input_1 = layers.Input(shape=input_shape)\n","    input_2 = layers.Input(shape=input_shape)\n","\n","    encoded_1 = base_model(input_1)\n","    encoded_2 = base_model(input_2)\n","\n","    # Calculate the Euclidean distance between the two encodings\n","    distance = layers.Lambda(lambda tensors: tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True))([encoded_1, encoded_2])\n","\n","    # Convert the distance to a similarity score\n","    similarity = layers.Dense(1, activation='sigmoid')(distance)\n","\n","    model = models.Model(inputs=[input_1, input_2], outputs=similarity)\n","    return model\n","\n","input_shape = (224, 224, 3)\n","contrastive_model = create_contrastive_model(input_shape)\n","\n","# Compile the model\n","contrastive_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","images_train, images_val = train_test_split(images, test_size=0.2, random_state=42)\n","\n","# Data Augmentation and Pairing\n","def augment_and_pair(image_1, image_2, label):\n","    # Augment both images in the pair\n","    augmented_1 = tf.image.random_flip_left_right(image_1)\n","    augmented_1 = tf.image.random_contrast(augmented_1, lower=0.8, upper=1.2)\n","    augmented_1 = tf.image.random_brightness(augmented_1, max_delta=0.2)\n","\n","    augmented_2 = tf.image.random_flip_left_right(image_2)\n","    augmented_2 = tf.image.random_contrast(augmented_2, lower=0.8, upper=1.2)\n","    augmented_2 = tf.image.random_brightness(augmented_2, max_delta=0.2)\n","\n","    return (augmented_1, augmented_2), label\n","\n","# Create a dataset with both similar and dissimilar pairs\n","def create_pairs(images):\n","    pairs = []\n","    labels = []\n","    for i in range(len(images)):\n","        # Create a positive pair (similar)\n","        j = random.choice(range(len(images)))  # Pick a random index for a similar pair\n","        pairs.append((images[i], images[j]))\n","        labels.append(1)\n","\n","        # Create a negative pair (dissimilar)\n","        k = random.choice(range(len(images)))  # Pick a random index for a dissimilar pair\n","        while k == i:\n","            k = random.choice(range(len(images)))\n","        pairs.append((images[i], images[k]))\n","        labels.append(0)\n","\n","    return pairs, labels\n","\n","def simclr_dataset(images, batch_size):\n","    pairs, labels = create_pairs(images)\n","    dataset = tf.data.Dataset.from_tensor_slices((pairs, labels))\n","    dataset = dataset.map(lambda x, y: augment_and_pair(x[0], x[1], y), num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","batch_size = 32\n","simclr_train_ds = simclr_dataset(images_train, batch_size)\n","simclr_val_ds = simclr_dataset(images_val, batch_size)\n","\n","# Callbacks for early stopping and saving the best model\n","early_stopping = callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","model_checkpoint = callbacks.ModelCheckpoint(\n","    filepath='/content/drive/MyDrive/Modelling/Checkpoints/best_contrastive_model.keras',\n","    monitor='val_loss',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","# Train the model\n","contrastive_model.fit(\n","    simclr_train_ds,\n","    epochs=50,\n","    validation_data=simclr_val_ds,\n","    callbacks=[early_stopping, model_checkpoint],\n","    verbose=1\n",")"],"metadata":{"id":"_Rk2LpJ8L53Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prediction Layer"],"metadata":{"id":"Jcy0mm24eayC"}},{"cell_type":"code","source":["base_model = contrastive_model.layers[2]  # Assuming the base model is the third layer\n","embeddings = base_model.predict(images_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVmoLD6CcfsH","executionInfo":{"status":"ok","timestamp":1723716362490,"user_tz":-120,"elapsed":4472,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}},"outputId":"40628521-e1e7-47e6-9c83-8fff35e51ee1"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"]}]},{"cell_type":"code","source":["!pip install xgb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWbvcwbkgP2A","executionInfo":{"status":"ok","timestamp":1723716410078,"user_tz":-120,"elapsed":2249,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}},"outputId":"d1de92ad-93c4-41db-ec67-299d44631135"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement xgb (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for xgb\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["def create_nn_regressor(input_shape):\n","    model = models.Sequential([\n","        layers.Input(shape=input_shape),\n","        layers.Dense(256, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(64, activation='relu'),\n","        layers.Dense(1, activation='linear')\n","    ])\n","    return model\n","\n","# Get the input shape from the embeddings\n","input_shape = embeddings.shape[1:]\n","\n","# Create the neural network regressor\n","nn_regressor = create_nn_regressor(input_shape)\n","\n","# Compile the model\n","nn_regressor.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","\n","# Train the model\n","nn_regressor.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping_callback])\n","\n","# Predict on the test set\n","y_pred = nn_regressor.predict(X_test)\n","\n","# Evaluate the model\n","from sklearn.metrics import r2_score\n","print(r2_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgjGxWJxdmFG","executionInfo":{"status":"ok","timestamp":1723716538572,"user_tz":-120,"elapsed":10469,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}},"outputId":"77d50712-1488-4cc9-f718-f0f6fb9d2222"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.9179 - mae: 0.8688 - val_loss: 1.3321 - val_mae: 0.6001\n","Epoch 2/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2201 - mae: 0.7420 - val_loss: 1.3347 - val_mae: 0.5814\n","Epoch 3/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2196 - mae: 0.5777 - val_loss: 1.3409 - val_mae: 0.6198\n","Epoch 4/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9556 - mae: 0.6658 - val_loss: 1.3314 - val_mae: 0.5975\n","Epoch 5/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1795 - mae: 0.6086 - val_loss: 1.3313 - val_mae: 0.5966\n","Epoch 6/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2946 - mae: 0.6025 - val_loss: 1.3327 - val_mae: 0.6020\n","Epoch 7/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5487 - mae: 0.6403 - val_loss: 1.3316 - val_mae: 0.5893\n","Epoch 8/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2189 - mae: 0.5893 - val_loss: 1.3360 - val_mae: 0.6084\n","Epoch 9/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8293 - mae: 0.6455 - val_loss: 1.3319 - val_mae: 0.5878\n","Epoch 10/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4194 - mae: 0.6160 - val_loss: 1.3331 - val_mae: 0.6033\n","Epoch 11/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1153 - mae: 0.6695 - val_loss: 1.3311 - val_mae: 0.5921\n","Epoch 12/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2508 - mae: 0.6144 - val_loss: 1.3316 - val_mae: 0.5890\n","Epoch 13/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6136 - mae: 0.6187 - val_loss: 1.3315 - val_mae: 0.5982\n","Epoch 14/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4099 - mae: 0.6849 - val_loss: 1.3310 - val_mae: 0.5935\n","Epoch 15/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4700 - mae: 0.6233 - val_loss: 1.3315 - val_mae: 0.5897\n","Epoch 16/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7122 - mae: 0.6656 - val_loss: 1.3326 - val_mae: 0.5857\n","Epoch 17/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5832 - mae: 0.6230 - val_loss: 1.3319 - val_mae: 0.5997\n","Epoch 18/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4029 - mae: 0.6271 - val_loss: 1.3334 - val_mae: 0.6039\n","Epoch 19/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3494 - mae: 0.6095 - val_loss: 1.3317 - val_mae: 0.5991\n","Epoch 20/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5802 - mae: 0.6861 - val_loss: 1.3312 - val_mae: 0.5913\n","Epoch 21/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5210 - mae: 0.5976 - val_loss: 1.3367 - val_mae: 0.6094\n","Epoch 22/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9290 - mae: 0.6418 - val_loss: 1.3317 - val_mae: 0.5992\n","Epoch 23/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0452 - mae: 0.5853 - val_loss: 1.3382 - val_mae: 0.6114\n","Epoch 24/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4049 - mae: 0.7036 - val_loss: 1.3314 - val_mae: 0.5902\n","Epoch 25/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1383 - mae: 0.5716 - val_loss: 1.3316 - val_mae: 0.5986\n","Epoch 26/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5963 - mae: 0.6745 - val_loss: 1.3312 - val_mae: 0.5961\n","Epoch 27/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2739 - mae: 0.5830 - val_loss: 1.3353 - val_mae: 0.6073\n","Epoch 28/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4573 - mae: 0.6336 - val_loss: 1.3315 - val_mae: 0.5983\n","Epoch 29/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8147 - mae: 0.6338 - val_loss: 1.3323 - val_mae: 0.6012\n","Epoch 30/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3390 - mae: 0.6066 - val_loss: 1.3330 - val_mae: 0.6030\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 40 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x780d7676bac0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","-0.0005467976552211429\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hj5PVzAvdmCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KUgIAwn3dl_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fYd5pHacdl9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uPIuqYI7dl2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/Modelling/saved_model/my_model.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMm2p16bNpBa","executionInfo":{"status":"ok","timestamp":1723713117621,"user_tz":-120,"elapsed":834,"user":{"displayName":"Chris Hild","userId":"16253994151537008663"}},"outputId":"e84018f6-3cd5-4859-a1cf-29adabbd6077"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ggrWYTQAbB6j"},"execution_count":null,"outputs":[]}]}