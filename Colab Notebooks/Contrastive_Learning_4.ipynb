{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D, Input, Flatten, Dense, Lambda, BatchNormalization, Dropout, GlobalAveragePooling2D, Concatenate, Activation, Add\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, save_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the labels\n",
        "labels_path = '/content/drive/MyDrive/Contrastive Learning/contrastive_learning_labels.csv'\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "\n",
        "# Directory path where images are stored\n",
        "image_dir = '/content/drive/MyDrive/Contrastive Learning/contrastive_learning_images'\n",
        "\n",
        "# Check if the image directory exists\n",
        "if not os.path.exists(image_dir):\n",
        "    print(\"Image directory not found!\")\n",
        "else:\n",
        "    print(\"Images are ready for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgrsq15mLMck",
        "outputId": "691114ab-fce3-4140-ed45-d40a7cf3e6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Images are ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DJUoZQEK6vy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60d1dc0-1639-4bf3-99e5-0a16c65c2f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 710ms/step - classifier_accuracy: 0.5395 - loss: 1.3719\n",
            "Epoch 2/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 1s/step - classifier_accuracy: 0.5310 - loss: 0.8797\n",
            "Epoch 3/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 671ms/step - classifier_accuracy: 0.5641 - loss: 0.7641\n",
            "Epoch 4/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 620ms/step - classifier_accuracy: 0.6063 - loss: 0.7106\n",
            "Epoch 5/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 625ms/step - classifier_accuracy: 0.5899 - loss: 0.6733\n",
            "Epoch 6/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 675ms/step - classifier_accuracy: 0.5632 - loss: 0.6977\n",
            "Epoch 7/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 620ms/step - classifier_accuracy: 0.5954 - loss: 0.6671\n",
            "Epoch 8/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 592ms/step - classifier_accuracy: 0.6123 - loss: 0.6475\n",
            "Epoch 9/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 630ms/step - classifier_accuracy: 0.5917 - loss: 0.6642\n",
            "Epoch 10/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 588ms/step - classifier_accuracy: 0.5854 - loss: 0.6798\n",
            "Epoch 11/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 606ms/step - classifier_accuracy: 0.6226 - loss: 0.6381\n",
            "Epoch 12/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 564ms/step - classifier_accuracy: 0.5862 - loss: 0.6807\n",
            "Epoch 13/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 585ms/step - classifier_accuracy: 0.6239 - loss: 0.6398\n",
            "Epoch 14/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 562ms/step - classifier_accuracy: 0.5799 - loss: 0.7055\n",
            "Epoch 15/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 608ms/step - classifier_accuracy: 0.5900 - loss: 0.6914\n",
            "Epoch 16/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 589ms/step - classifier_accuracy: 0.6422 - loss: 0.6388\n",
            "Epoch 17/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - classifier_accuracy: 0.6117 - loss: 0.6668\n",
            "Epoch 18/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 616ms/step - classifier_accuracy: 0.6038 - loss: 0.6591\n",
            "Epoch 19/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 610ms/step - classifier_accuracy: 0.6446 - loss: 0.6272\n",
            "Epoch 20/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 560ms/step - classifier_accuracy: 0.6507 - loss: 0.6308\n",
            "Epoch 21/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 651ms/step - classifier_accuracy: 0.6008 - loss: 0.6618\n",
            "Epoch 22/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 606ms/step - classifier_accuracy: 0.6696 - loss: 0.6019\n",
            "Epoch 23/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 623ms/step - classifier_accuracy: 0.5920 - loss: 0.6636\n",
            "Epoch 24/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 593ms/step - classifier_accuracy: 0.6689 - loss: 0.6041\n",
            "Epoch 25/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 568ms/step - classifier_accuracy: 0.6135 - loss: 0.6557\n",
            "Epoch 26/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 611ms/step - classifier_accuracy: 0.6375 - loss: 0.6248\n",
            "Epoch 27/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 641ms/step - classifier_accuracy: 0.6473 - loss: 0.6268\n",
            "Epoch 28/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 612ms/step - classifier_accuracy: 0.6126 - loss: 0.6725\n",
            "Epoch 29/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 642ms/step - classifier_accuracy: 0.6347 - loss: 0.6538\n",
            "Epoch 30/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 608ms/step - classifier_accuracy: 0.6631 - loss: 0.6064\n"
          ]
        }
      ],
      "source": [
        "def residual_block(x, filters, kernel_size=3, stride=1, activation='relu', dropout_rate=0.3):\n",
        "    res = Conv2D(filters, kernel_size, strides=stride, padding='same', activation=activation)(x)\n",
        "    res = BatchNormalization()(res)\n",
        "    res = Dropout(dropout_rate)(res)\n",
        "\n",
        "    res = Conv2D(filters, kernel_size, strides=1, padding='same', activation=None)(res)\n",
        "    res = BatchNormalization()(res)\n",
        "\n",
        "    if x.shape[-1] != filters:\n",
        "        x = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same', activation=None)(x)\n",
        "\n",
        "    res = Add()([x, res])\n",
        "    res = Activation(activation)(res)\n",
        "    return res\n",
        "\n",
        "def make_embedding_model(input_shape=(224, 224, 3), scalar_shape=(2,), embedding_dim=128, dropout_rate=0.3):\n",
        "    inp = Input(shape=input_shape, name='input_image')\n",
        "\n",
        "    c1 = Conv2D(32, (7, 7), strides=2, activation='relu', padding='same', name='conv_layer_1')(inp)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    m1 = MaxPooling2D((3, 3), strides=2, padding='same', name='pool_layer_1')(c1)\n",
        "\n",
        "    r1 = residual_block(m1, 64, dropout_rate=dropout_rate)\n",
        "    r2 = residual_block(r1, 128, dropout_rate=dropout_rate)\n",
        "    r3 = residual_block(r2, 256, dropout_rate=dropout_rate)\n",
        "\n",
        "    f1 = GlobalAveragePooling2D(name='global_avg_pool')(r3)\n",
        "\n",
        "    scalar_input = Input(shape=scalar_shape, name='input_scalar')\n",
        "    scalar_dense = Dense(64, activation='relu')(scalar_input)\n",
        "    scalar_dense = BatchNormalization()(scalar_dense)\n",
        "    scalar_dense = Dropout(dropout_rate)(scalar_dense)\n",
        "\n",
        "    combined = Concatenate(name='concat_image_scalar')([f1, scalar_dense])\n",
        "\n",
        "    dense_1 = Dense(256, activation='relu', name='dense_layer_1')(combined)\n",
        "    dense_1 = BatchNormalization()(dense_1)\n",
        "    dense_1 = Dropout(dropout_rate)(dense_1)\n",
        "\n",
        "    embedding = Dense(embedding_dim, name='embedding_layer')(dense_1)\n",
        "    embedding = BatchNormalization(name='embedding_batch_norm')(embedding)\n",
        "\n",
        "    return Model(inputs=[inp, scalar_input], outputs=embedding, name='embedding_model')\n",
        "\n",
        "# Adjust the data generator to include both scalar features (pothole_area_mm2 and mm_to_pixel_ratio)\n",
        "def data_generator(save_dir, batch_size=32, augment=False):\n",
        "    pairs_batches = sorted([f for f in os.listdir(save_dir) if f.startswith('pairs_batch_') and f.endswith('.npy')])\n",
        "    scalar_features_1_batches = sorted([f for f in os.listdir(save_dir) if f.startswith('scalar_features_1_batch_') and f.endswith('.npy')])\n",
        "    scalar_features_2_batches = sorted([f for f in os.listdir(save_dir) if f.startswith('scalar_features_2_batch_') and f.endswith('.npy')])\n",
        "    labels_batches = sorted([f for f in os.listdir(save_dir) if f.startswith('labels_batch_') and f.endswith('.npy')])\n",
        "\n",
        "    while True:\n",
        "        # Shuffle the batches together\n",
        "        combined_batches = list(zip(pairs_batches, scalar_features_1_batches, scalar_features_2_batches, labels_batches))\n",
        "        np.random.shuffle(combined_batches)\n",
        "\n",
        "        for pair_file, sf1_file, sf2_file, label_file in combined_batches:\n",
        "            pairs = np.load(os.path.join(save_dir, pair_file))\n",
        "            scalar_features_1 = np.load(os.path.join(save_dir, sf1_file))  # Include both area and ratio\n",
        "            scalar_features_2 = np.load(os.path.join(save_dir, sf2_file))  # Include both area and ratio\n",
        "            labels = np.load(os.path.join(save_dir, label_file))\n",
        "\n",
        "            num_batches = len(pairs) // batch_size\n",
        "            for i in range(num_batches):\n",
        "                batch_pairs = pairs[i * batch_size:(i + 1) * batch_size]\n",
        "                batch_sf1 = scalar_features_1[i * batch_size:(i + 1) * batch_size]\n",
        "                batch_sf2 = scalar_features_2[i * batch_size:(i + 1) * batch_size]\n",
        "                batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "                if augment:\n",
        "                    batch_pairs[:, 0] = data_augmentation(batch_pairs[:, 0])\n",
        "                    batch_pairs[:, 1] = data_augmentation(batch_pairs[:, 1])\n",
        "\n",
        "                # Yield the correctly shaped data\n",
        "                yield (\n",
        "                    (\n",
        "                        tf.convert_to_tensor(batch_pairs[:, 0], dtype=tf.float32),\n",
        "                        tf.convert_to_tensor(batch_sf1, dtype=tf.float32),\n",
        "                        tf.convert_to_tensor(batch_pairs[:, 1], dtype=tf.float32),\n",
        "                        tf.convert_to_tensor(batch_sf2, dtype=tf.float32)\n",
        "                    ),\n",
        "                    tf.convert_to_tensor(batch_labels, dtype=tf.float32)\n",
        "                )\n",
        "\n",
        "# Define the input shapes\n",
        "image_shape = (224, 224, 3)\n",
        "scalar_shape = (2,)  # Two scalar inputs: pothole_area_mm2 and mm_to_pixel_ratio\n",
        "\n",
        "# Load the embedding model with scalar features included\n",
        "base_network = make_embedding_model(input_shape=image_shape, scalar_shape=scalar_shape, embedding_dim=128)\n",
        "\n",
        "# Input tensors for the two images and scalar features\n",
        "input_a = Input(shape=image_shape, name='input_img_a')\n",
        "input_b = Input(shape=image_shape, name='input_img_b')\n",
        "\n",
        "scalar_input_a = Input(shape=scalar_shape, name='input_scalar_a')\n",
        "scalar_input_b = Input(shape=scalar_shape, name='input_scalar_b')\n",
        "\n",
        "# Generate embeddings for both inputs (image + scalar features)\n",
        "embedding_a = base_network([input_a, scalar_input_a])\n",
        "embedding_b = base_network([input_b, scalar_input_b])\n",
        "\n",
        "# Calculate the distance between the embeddings\n",
        "distance_layer = Lambda(lambda x: tf.math.square(x[0] - x[1]), name='distance_layer')\n",
        "distance_output = distance_layer([embedding_a, embedding_b])\n",
        "\n",
        "# Add classification layer\n",
        "classifier = Dense(1, activation='sigmoid', name='classifier')(distance_output)\n",
        "\n",
        "# Define the contrastive model with scalar inputs\n",
        "model = Model(inputs=[input_a, scalar_input_a, input_b, scalar_input_b], outputs=[classifier, distance_output])\n",
        "\n",
        "# Define the contrastive loss function\n",
        "class ContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        label = tf.cast(y_true, tf.float32)\n",
        "        neg_dist = tf.maximum(self.margin - y_pred, 0)\n",
        "        return tf.reduce_mean(label * y_pred + (1.0 - label) * neg_dist, axis=-1)\n",
        "\n",
        "# Instantiate loss functions\n",
        "loss_contrastive = ContrastiveLoss(margin=1.0)\n",
        "loss_classifier = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Set a constant learning rate\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=[loss_classifier, loss_contrastive],\n",
        "    optimizer=optimizer,\n",
        "    loss_weights=[1.0, 1.0],\n",
        "    metrics=[['accuracy'], []]  # accuracy for classifier, no metrics for the contrastive loss\n",
        ")\n",
        "\n",
        "# Training dataset (no validation dataset)\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(save_dir='/content/drive/MyDrive/pairs_batches', batch_size=32, augment=True),\n",
        "    output_signature=(\n",
        "        (\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),  # Input image A\n",
        "            tf.TensorSpec(shape=(None, 2), dtype=tf.float32),            # Scalar features A (area and ratio)\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),  # Input image B\n",
        "            tf.TensorSpec(shape=(None, 2), dtype=tf.float32)             # Scalar features B (area and ratio)\n",
        "        ),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.float32)                  # Labels\n",
        "    )\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,  # Run for 30 epochs\n",
        "    steps_per_epoch=100,  # Adjust this based on the size of your dataset\n",
        "    verbose=1  # Print training progress\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_network.save('/content/drive/MyDrive/Contrastive Learning/embedding_model_with_scalars.keras')"
      ],
      "metadata": {
        "id": "qKwlHHi7Pj8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_vdhtBsZJMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}