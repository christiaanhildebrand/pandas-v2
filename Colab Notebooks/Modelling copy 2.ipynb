{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNuUJoqSSxtyUMn5ctwqzX6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4RHrdA5KYAv","executionInfo":{"status":"ok","timestamp":1723717673062,"user_tz":-120,"elapsed":3537,"user":{"displayName":"Chris Hild","userId":"14260243667832439949"}},"outputId":"ec2dd994-b3c6-4afe-ec77-826b81f02c4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers, models, callbacks\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.losses import Huber\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import random\n","import pickle\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load your train_df\n","train_df = pd.read_csv('/content/drive/MyDrive/Modelling/train_df.csv')  # Change to your path\n"]},{"cell_type":"code","source":["def load_and_preprocess_image(image_path, target_size=(224, 224)):\n","    image = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n","    image = tf.keras.preprocessing.image.img_to_array(image)\n","    image = image / 255.0  # Normalize to [0, 1]\n","    return image\n","\n","# Prepare image data and scalar features\n","image_dir = '/content/drive/MyDrive/Modelling/valid_images'\n","image_paths = []\n","scalar_features = []\n","labels = []\n","\n","for index, row in train_df.iterrows():\n","    pothole_id = row['pothole_id']\n","    image_path = os.path.join(image_dir, f'{pothole_id}.jpg')\n","\n","    if os.path.exists(image_path):\n","        image_paths.append(image_path)\n","        scalar_features.append([row['pothole_area_mm2']])\n","        labels.append(row['bags_used'])\n","\n","# Convert lists to numpy arrays\n","images = np.array([load_and_preprocess_image(path) for path in image_paths])\n","scalar_features = np.array(scalar_features)\n","labels = np.array(labels)\n","\n","# Scale the scalar features\n","scaler = StandardScaler()\n","scalar_features = scaler.fit_transform(scalar_features)\n","\n","# Save the scaler for later use\n","scaler_path = '/content/drive/MyDrive/Modelling/scaler.pkl'\n","with open(scaler_path, 'wb') as f:\n","    pickle.dump(scaler, f)"],"metadata":{"id":"GvrSyA1SjcxH","executionInfo":{"status":"ok","timestamp":1723717621522,"user_tz":-120,"elapsed":342094,"user":{"displayName":"Chris Hild","userId":"14260243667832439949"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Assuming `images_train` and `images_val` are predefined\n","def augment_and_pair(image_1, image_2, label):\n","    # Augment both images in the pair\n","    augmented_1 = tf.image.random_flip_left_right(image_1)\n","    augmented_1 = tf.image.random_contrast(augmented_1, lower=0.8, upper=1.2)\n","    augmented_1 = tf.image.random_brightness(augmented_1, max_delta=0.2)\n","\n","    augmented_2 = tf.image.random_flip_left_right(image_2)\n","    augmented_2 = tf.image.random_contrast(augmented_2, lower=0.8, upper=1.2)\n","    augmented_2 = tf.image.random_brightness(augmented_2, max_delta=0.2)\n","\n","    return (augmented_1, augmented_2), label\n","\n","# Create a dataset with both similar and dissimilar pairs\n","def create_pairs(images):\n","    pairs = []\n","    labels = []\n","    for i in range(len(images)):\n","        # Create a positive pair (similar)\n","        j = random.choice(range(len(images)))  # Pick a random index for a similar pair\n","        pairs.append((images[i], images[j]))\n","        labels.append(1)\n","\n","        # Create a negative pair (dissimilar)\n","        k = random.choice(range(len(images)))  # Pick a random index for a dissimilar pair\n","        while k == i:\n","            k = random.choice(range(len(images)))\n","        pairs.append((images[i], images[k]))\n","        labels.append(0)\n","\n","    return pairs, labels\n","\n","# Generate pairs for training and validation\n","pairs_train, labels_train = create_pairs(images_train)\n","pairs_val, labels_val = create_pairs(images_val)\n","\n","def simclr_dataset(pairs, labels, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices((pairs, labels))\n","    dataset = dataset.map(lambda x, y: augment_and_pair(x[0], x[1], y), num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","batch_size = 32\n","simclr_train_ds = simclr_dataset(pairs_train, labels_train, batch_size)\n","simclr_val_ds = simclr_dataset(pairs_val, labels_val, batch_size)"],"metadata":{"id":"ja5jqylTjey2","executionInfo":{"status":"ok","timestamp":1723718703793,"user_tz":-120,"elapsed":449,"user":{"displayName":"Chris Hild","userId":"14260243667832439949"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Improved Complex CNN Model\n","def create_complex_cnn_model(input_shape):\n","    model = models.Sequential([\n","        layers.Input(shape=input_shape),\n","        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n","        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n","        layers.MaxPooling2D((2, 2)),\n","\n","        layers.GlobalAveragePooling2D(),\n","        layers.Dense(512, activation='relu'),\n","        layers.Dropout(0.5),  # Add dropout to reduce overfitting\n","        layers.Dense(256, activation='relu'),\n","    ])\n","    return model\n","\n","def create_contrastive_model(input_shape):\n","    base_model = create_complex_cnn_model(input_shape)\n","\n","    input_1 = layers.Input(shape=input_shape)\n","    input_2 = layers.Input(shape=input_shape)\n","\n","    encoded_1 = base_model(input_1)\n","    encoded_2 = base_model(input_2)\n","\n","    # Calculate the Euclidean distance between the two encodings\n","    distance = layers.Lambda(lambda tensors: tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True))([encoded_1, encoded_2])\n","\n","    # Convert the distance to a similarity score\n","    similarity = layers.Dense(1, activation='sigmoid')(distance)\n","\n","    model = models.Model(inputs=[input_1, input_2], outputs=similarity)\n","    return model\n","\n","input_shape = (224, 224, 3)\n","contrastive_model = create_contrastive_model(input_shape)\n","\n","# Compile the model\n","contrastive_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Callbacks for early stopping and saving the best model\n","early_stopping = callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","model_checkpoint = callbacks.ModelCheckpoint(\n","    filepath='/content/drive/MyDrive/Modelling/Checkpoints/best_contrastive_model.keras',\n","    monitor='val_loss',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","# Train the model\n","contrastive_model.fit(\n","    simclr_train_ds,\n","    epochs=50,\n","    validation_data=simclr_val_ds,\n","    callbacks=[early_stopping, model_checkpoint],\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBqBt8mYjnMm","outputId":"13f835a4-ce65-4d3f-f776-ebe7ae7e08f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692ms/step - accuracy: 0.4972 - loss: 0.6933\n","Epoch 1: val_loss improved from inf to 0.69319, saving model to /content/drive/MyDrive/Modelling/Checkpoints/best_contrastive_model.keras\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 912ms/step - accuracy: 0.4971 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6932\n","Epoch 2/50\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.5093 - loss: 0.6931\n","Epoch 2: val_loss did not improve from 0.69319\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 685ms/step - accuracy: 0.5091 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6933\n","Epoch 3/50\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634ms/step - accuracy: 0.4942 - loss: 0.6933\n","Epoch 3: val_loss did not improve from 0.69319\n","\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 686ms/step - accuracy: 0.4944 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6932\n","Epoch 4/50\n","\u001b[1m29/51\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 640ms/step - accuracy: 0.4829 - loss: 0.6931"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OP-j0A8joa-5"},"execution_count":null,"outputs":[]}]}